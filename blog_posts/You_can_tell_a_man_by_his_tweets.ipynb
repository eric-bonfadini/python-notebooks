{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some code snippets I used in writing the following blog post:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some time ago I read a tweet of Kenneth Reitz, a very well known Python developer I follow on Twitter, asking:\n",
    "{% twitter 952553176925958145 %}\n",
    "Starting from this, I decided to analyze some tweets from pretty popular Python devs in order to understand a priori how they use Twitter, what they tweet about and what I can gather using data from Twitter APIs only.\n",
    "Obviously you can apply the same analysis on a different list of Twitter accounts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my analysis I set up a Python 3.6 virtual environment with the following main libraries:\n",
    "- [Tweepy](https://github.com/tweepy/tweepy) for interaction with Twitter APIs\n",
    "- [Pandas](https://pandas.pydata.org/) for data analysis and visualization\n",
    "- [Beautiful Soup 4](https://pypi.org/project/beautifulsoup4/), [NLTK](https://www.nltk.org/) and [Gensim](https://radimrehurek.com/gensim/) for processing text data\n",
    "\n",
    "Some extra libraries will be introduced later on, along with the explanation of the steps I did.\n",
    "\n",
    "In order to access the Twitter APIs I [registered my app](https://apps.twitter.com/) and then I provided the tweepy library with the consumer_key, access_token, access_token_secret and consumer_secret.\n",
    "\n",
    "We're now ready to get some real data from Twitter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import ssl\n",
    "import timeit\n",
    "from collections import Counter\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import tweepy\n",
    "from IPython.display import Markdown, display\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel, AuthorTopicModel, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from geopy import Nominatim\n",
    "from matplotlib import style, ticker, rcParams\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from configs import twitter_consumer_key, twitter_access_token, twitter_access_token_secret, twitter_consumer_secret\n",
    "\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "%matplotlib inline\n",
    "rcParams.update({'font.size': 13})\n",
    "rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.WARN)\n",
    "logging.root.level = logging.WARN\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Choosing a list of Twitter accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, I chose a list of 8 popular Python devs, starting from [the top 360 most-downloaded packages on PyPI](https://pythonwheels.com/) and selecting some libraries I know or use daily.\n",
    "\n",
    "Here's my final list, including links to the Twitter account and the libraries (from the above mentioned list) for which those guys are known for:\n",
    "- [@kennethreitz](https://twitter.com/kennethreitz): requests\n",
    "- [@mitsuhiko](https://twitter.com/mitsuhiko): jinja2/flask\n",
    "- [@zzzeek](https://twitter.com/zzzeek): sqlalchemy\n",
    "- [@teoliphant](https://twitter.com/teoliphant): numpy/scipy/numba\n",
    "- [@benoitc](https://twitter.com/benoitc): gunicorn\n",
    "- [@asksol](https://twitter.com/asksol): celery\n",
    "- [@wesmckinn](https://twitter.com/wesmckinn): pandas\n",
    "- [@cournape](https://twitter.com/cournape): scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data from Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got all the data with two endpoints only:\n",
    "- with a call to [lookup users](https://developer.twitter.com/en/docs/accounts-and-users/follow-search-get-users/api-reference/get-users-lookup) I could get all the information about the accounts (creation date, description, counts, location, etc.)\n",
    "- with a call to [user timeline](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline.html) I could get the tweets about a single user and all the information related to every single tweet. I configured the call to get also retweets and replies.\n",
    "\n",
    "I saved the results from the two calls in two Pandas dataframes in order to ease the data processing and then into CSV files to be used as starting point for the next steps without re-calling each time the Twitter API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_twitter_api() -> tweepy.api:\n",
    "    auth = tweepy.OAuthHandler(twitter_consumer_key, twitter_consumer_secret)\n",
    "    auth.set_access_token(twitter_access_token, twitter_access_token_secret)\n",
    "\n",
    "    return tweepy.API(auth)\n",
    "\n",
    "\n",
    "def _print_api_limits(api: tweepy.api, api_endpoint: str) -> int:\n",
    "\n",
    "    limits = api.rate_limit_status()\n",
    "\n",
    "    endpoint_limits = limits[\"resources\"][\"statuses\"][f\"/statuses/{api_endpoint}\"]\n",
    "    usertimeline_remain = endpoint_limits[\"remaining\"]\n",
    "    usertimeline_limit = endpoint_limits[\"limit\"]\n",
    "    usertimeline_time_local = datetime.fromtimestamp(endpoint_limits[\"reset\"])\n",
    "    print(f\"Twitter API for {api_endpoint}: remain: {usertimeline_remain} / limit: {usertimeline_limit} - resetting at: {usertimeline_time_local}\")\n",
    "    return usertimeline_remain\n",
    "\n",
    "\n",
    "def _get_df_users(api: tweepy.api, accounts: List[str]) -> pd.DataFrame:\n",
    "\n",
    "    user_res = api.lookup_users(screen_names=accounts)\n",
    "    df_users = pd.DataFrame([res._json for res in user_res])[[\"created_at\", \"description\", \"favourites_count\",\n",
    "                                                              \"followers_count\", \"friends_count\", \"id_str\",\n",
    "                                                              \"listed_count\", \"location\", \"name\", \"screen_name\",\n",
    "                                                              \"statuses_count\", \"time_zone\", \"verified\"]]\n",
    "    df_users.rename(columns={\"friends_count\": \"following_count\", \"statuses_count\": \"total_tweets\"}, inplace=True)\n",
    "\n",
    "    return df_users\n",
    "\n",
    "\n",
    "def _get_df_tweets(api: tweepy.api, account: str) -> pd.DataFrame:\n",
    "\n",
    "    count = 200\n",
    "    include_rts = True\n",
    "    exclude_replies = False\n",
    "    tweet_mode = \"extended\"\n",
    "\n",
    "    print(f\"Getting tweets of [{account}]\")\n",
    "    fields = (\"created_at\", \"entities\", \"favorite_count\", \"id_str\", \"lang\", \"possibly_sensitive\", \"retweet_count\",\n",
    "              \"full_text\", \"truncated\", \"source\", \"geo\", \"coordinates\", \"place\",\n",
    "              \"in_reply_to_status_id\", \"retweeted_status\", )\n",
    "    tweets = []\n",
    "    \n",
    "    new_tweets = api.user_timeline(screen_name=account, count=count, include_rts=include_rts,\n",
    "                                   exclude_replies=exclude_replies, tweet_mode=tweet_mode)\n",
    "\n",
    "    while len(new_tweets) > 0:\n",
    "        for tweet_res in new_tweets:\n",
    "            items = {k: v for k, v in tweet_res._json.items() if k in fields}\n",
    "            tweets.append(items)\n",
    "\n",
    "        print(f\"We've got {len(tweets)} tweets so far for user @{account}\")\n",
    "        max_id = new_tweets[-1].id - 1\n",
    "        new_tweets = api.user_timeline(screen_name=account, count=count, max_id=max_id, include_rts=include_rts,\n",
    "                                       exclude_replies=exclude_replies, tweet_mode=tweet_mode)\n",
    "\n",
    "    df = pd.DataFrame(tweets)\n",
    "    df[\"username\"] = account\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accounts = [\n",
    "        \"kennethreitz\",  # requests\n",
    "        \"mitsuhiko\",  # jinja2/flask\n",
    "        \"zzzeek\",  # sqlalchemy\n",
    "        \"teoliphant\",  # numpy/scipy/numba\n",
    "        \"benoitc\",  # gunicorn\n",
    "        \"asksol\",  # celery\n",
    "        \"wesmckinn\",  # pandas\n",
    "        \"cournape\",  # scikit-learn\n",
    "    ]\n",
    "\n",
    "api = _get_twitter_api()\n",
    "\n",
    "_print_api_limits(api, \"user_timeline\")\n",
    "\n",
    "df_tweets = pd.DataFrame()\n",
    "\n",
    "df_users = _get_df_users(api=api, accounts=accounts)\n",
    "\n",
    "display(df_users)\n",
    "\n",
    "for account in accounts:\n",
    "\n",
    "    _print_api_limits(api, \"user_timeline\")\n",
    "\n",
    "    df = _get_df_tweets(api=api, account=account)\n",
    "\n",
    "    df_tweets = df_tweets.append(df)\n",
    "\n",
    "display(df_tweets.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The users dataframe contained all the information I needed; I just created three more columns:\n",
    "- a followers/following ratio, a sort of \"popularity\" indicator\n",
    "- a tweets per day ratio, dividing the total number of tweets by the number of days since the creation of the account\n",
    "- the coordinates starting from the location, if available, using [Geopy](https://pypi.org/project/geopy/). @benoitc doesn't have a location, while @zzzeek has a generic \"northeast\", geolocated in Nebraska :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users[\"created_at\"] = pd.to_datetime(df_users[\"created_at\"])\n",
    "df_users[\"followers/following\"] = df_users[\"followers_count\"] / df_users[\"following_count\"]\n",
    "df_users[\"tweets_per_day\"] = df_users[\"total_tweets\"] / (datetime.now().date() - df_users[\"created_at\"]).dt.days\n",
    "geolocator = Nominatim()\n",
    "df_users[\"location_coo\"] = df_users[\"location\"].apply(lambda x: [geolocator.geocode(x).latitude,\n",
    "                                                                 geolocator.geocode(x).longitude] if pd.notnull(x) and x not in (None, '') else x)\n",
    "display(df_users[[\"screen_name\", \"name\", \"verified\", \"description\", \"created_at\", \"location\", \"location_coo\", \n",
    "                  \"time_zone\", \"total_tweets\", \"favourites_count\", \"followers_count\", \"following_count\", \n",
    "                  \"listed_count\", \"followers/following\", \"tweets_per_day\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets dataframe on the contrary needed some extra preprocessing.\n",
    "\n",
    "First of all, I discovered an annoying limitation about the Twitter user timeline API: there's a maximum number of tweets that can be returned (more or less 3200 including retweets and replies). Therefore I decided to group the tweets by username and to get the oldest tweet date for each user.\n",
    "\n",
    "Then I filtered out all the tweets before the maximum value between the first dates (2017-08-26 20:48:35).\n",
    "Starting from these data, @kennethreitz is influencing the cut date because he's tweeting a lot more than some other users; but in this way we can at least get the same timeframe for all the users and compare tweets from the same period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_condition(df: pd.DataFrame, condition: pd.core.series.Series, label: str) -> pd.DataFrame:\n",
    "    before = len(df)\n",
    "    df = df[condition]\n",
    "    print(f\"{label}: {before}-{before-len(df)}={len(df)}\")\n",
    "    return df\n",
    "\n",
    "df_tweets[\"created_at\"] = pd.to_datetime(df_tweets[\"created_at\"])\n",
    "\n",
    "display(df_tweets.groupby(\"username\").agg({\"created_at\": [\"count\", \"min\", \"max\"]})[\"created_at\"].reset_index())\n",
    "max_min_created = max(df_tweets.groupby(\"username\").agg({\"created_at\": [\"min\"]})[\"created_at\"][\"min\"])\n",
    "df_tweets = _check_condition(df_tweets,\n",
    "                             condition=df_tweets[\"created_at\"] >= max_min_created,\n",
    "                             label=\"max min_created\")\n",
    "display(df_tweets.groupby(\"username\").agg({\"created_at\": [\"count\", \"min\", \"max\"]})[\"created_at\"].reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other preprocessing steps:\n",
    "\n",
    "- I parsed the source information using Beautiful Soup because it contained HTML entities\n",
    "- I removed smart quotes from text\n",
    "- I converted the text to lower case\n",
    "- I removed Twitter entities, URLs and numbers\n",
    "- I filtered out all the tweets with empty text after these steps (i.e. containing only mentions, etc) and I got 10948-217=10731 tweets.\n",
    "\n",
    "I finally created a new column containing the \"tweet type\" (standard, reply or retweet) and another column with the tweet length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "# Format source and replace smart quotes\n",
    "df_tweets[\"source\"] = df_tweets[\"source\"].map(lambda x: BeautifulSoup(x, \"lxml\").a.string if pd.notnull(x) else x)\n",
    "df_tweets[\"full_text\"] = df_tweets[\"full_text\"].str.replace(\"â\", \"'\")\n",
    "df_tweets[\"full_text\"] = df_tweets[\"full_text\"].str.replace(\"â\", '\"')\n",
    "df_tweets[\"full_text\"] = df_tweets[\"full_text\"].str.replace(\"â\", '\"')\n",
    "df_tweets[\"full_text\"] = df_tweets[\"full_text\"].map(lambda x: BeautifulSoup(x, \"lxml\").string if pd.notnull(x) else x)\n",
    "display(df_tweets[['username', 'created_at', 'full_text', 'lang', 'favorite_count', \n",
    "                   'retweet_count', 'source']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_tweet(tweet: pd.core.series.Series, remove_entities: bool = False) -> str:\n",
    "\n",
    "    # lowercase\n",
    "    tweet_text = tweet[\"full_text\"].lower()\n",
    "\n",
    "    # remove indices\n",
    "    if remove_entities:\n",
    "        indexes = []\n",
    "        for entity, values_list in eval(tweet.entities).items():\n",
    "            if values_list:\n",
    "                for val in values_list:\n",
    "                    indexes.append(val.get(\"indices\"))\n",
    "\n",
    "        for idx in sorted(indexes, reverse=True):\n",
    "            tweet_text = tweet_text[:idx[0]] + tweet_text[idx[1]:]\n",
    "\n",
    "    # remove remaining urls\n",
    "    tweet_text = re.sub(\"http\\\\S+\", \"\", tweet_text)\n",
    "\n",
    "    # only letters\n",
    "    tweet_text = re.sub(\"[^@#a-zA-Z]\", \" \", tweet_text)\n",
    "\n",
    "    tweet_text = \" \".join(tweet_text.split())\n",
    "\n",
    "    return tweet_text\n",
    "\n",
    "# Clean text\n",
    "df_tweets[\"text_clean\"] = df_tweets.apply(lambda x: _clean_tweet(x), axis=1)\n",
    "df_tweets = _check_condition(df_tweets,\n",
    "                             condition=df_tweets[\"text_clean\"] != \"\",\n",
    "                             label=\"empty text clean\")\n",
    "\n",
    "display(df_tweets[['username', 'created_at', 'full_text', 'lang', 'favorite_count', \n",
    "                   'retweet_count', 'source', 'text_clean']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # tweet type\n",
    "df_tweets['tweet_type'] = \"standard\"\n",
    "df_tweets['tweet_type'] = np.where(pd.notnull(df_tweets['in_reply_to_status_id']), \"reply\", df_tweets['tweet_type'])\n",
    "df_tweets['tweet_type'] = np.where(pd.notnull(df_tweets['retweeted_status']), \"retweet\", df_tweets['tweet_type'])\n",
    "\n",
    "display(df_tweets[['username', 'created_at', 'full_text', 'lang', 'favorite_count', \n",
    "                   'retweet_count', 'source', 'text_clean', 'tweet_type']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['tweet_len'] = df_tweets['full_text'].str.len()\n",
    "\n",
    "display(df_tweets[['username', 'created_at', 'full_text', 'lang', 'favorite_count', \n",
    "                   'retweet_count', 'source', 'text_clean', 'tweet_type', 'tweet_len']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorative Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The users dataframe itself already shows some insights:\n",
    "\n",
    "- There are only two accounts with the verified flag: @mitsuhiko and @wesmckinn\n",
    "- @wesmckinn, @kennethreitz, @teoliphant and @mitsuhiko are the most popular accounts in the list (according to my \"popularity\" indicator):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bar_plot(df: pd.DataFrame, show_graphs: bool, **kwargs):\n",
    "    print(f\"bar - {kwargs.get('title', '')}\")\n",
    "    if show_graphs:\n",
    "        df.plot.bar(**kwargs)\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.clf()\n",
    "\n",
    "show_graphs = True\n",
    "        \n",
    "kwargs = dict(legend=False, rot=45, title='Popularity indicator', x='screen_name', y='followers/following')\n",
    "_bar_plot(df=df_users, show_graphs=show_graphs, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- @kennethreitz wrote since the creation of his account at least twice the number of tweets per day compared to the other devs in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(legend=False, rot=45, title='Tweets per day', x='screen_name', y='tweets_per_day')\n",
    "_bar_plot(df=df_users, show_graphs=show_graphs, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the accounts in the list live in the US; I used Folium to create a map showing the locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(zoom_start=5)\n",
    "for _, row in df_users.iterrows():\n",
    "    if row[\"location_coo\"]:\n",
    "        folium.Marker(row[\"location_coo\"], popup=row[\"screen_name\"]).add_to(m)\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets dataframe needs instead some manipulation before we can gather some good insights.\n",
    "\n",
    "First of all let's check the tweet \"style\" of each account. From the following chart we can see for example that @cournape is retweeting a lot, while @mitsuhiko is replying a lot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.groupby(['username', 'tweet_type']).size().unstack().plot(kind='bar', rot=45, title='Tweet types')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also group by username and tweet type, and show a chart with the mean tweet length. @kennethreitz for example writes replies shorter than standard tweets, while @teoliphant writes tweets longer than the other guys (exceeding the 140 chars limit):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_tweets.groupby(['username', 'tweet_type']).agg({'tweet_len': [\"mean\"]}).unstack().plot(kind='bar', rot=45, title='Tweet length')\n",
    "ax.legend([\"reply\", \"retweet\", \"standard\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's filter out the retweets and let's focus on the machine-detected language used in standard tweets and replies. The five most common languages are: English, German, French, undefined and a rather weird \"tagalog\" (ISO 639-1 code \"tl\", maybe an error in auto-detection?). Most of the tweets are in English; @mitsuhiko tweets a lot in German, while @benoitrc in French:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = _check_condition(df_tweets,\n",
    "                                 condition=~df_tweets[\"tweet_type\"].isin([\"retweet\"]),\n",
    "                                 label=\"not RTs\")\n",
    "\n",
    "# Check languages and Keep only english tweets\n",
    "languages = df_tweets[\"lang\"].values.tolist()\n",
    "mc_languages = [l[0] for l in Counter(languages).most_common(5)]\n",
    "print(mc_languages)\n",
    "df_tweets[\"lang\"] = df_tweets[\"lang\"].where(df_tweets[\"lang\"].isin(mc_languages), 'other')\n",
    "df_tweets.groupby(['username', 'lang']).size().unstack().plot(kind='bar', rot=45, title='Languages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's just select tweets in English or undefined: all the next charts are just considering tweets and replies in English (but obviously you can tune differently your analysis).\n",
    "Let's group by username and get statistics about the number of favorites/retweets per user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = _check_condition(df_tweets,\n",
    "                                 condition=df_tweets[\"lang\"].isin([\"en\", \"und\"]),\n",
    "                                 label=\"only english/undefined\")\n",
    "\n",
    "# General stats\n",
    "tmp_df = df_tweets.groupby(\"username\").agg({\"favorite_count\": [\"count\", \"max\", \"mean\", \"std\"],\n",
    "                                       \"retweet_count\": [\"max\", \"mean\", \"std\"]}).reset_index()\n",
    "tmp_df.columns = [' '.join(col).strip() for col in tmp_df.columns.values]\n",
    "\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table we can see that:\n",
    "\n",
    "- @kennethreitz has the most retweeted and favorited tweet in the dataframe. Here's the [tweet](https://twitter.com/kennethreitz/status/981547972239417345)\n",
    "- @wesmckinn has the second most retweeted and favorited tweet in the dataframe. Here's the [tweet](https://twitter.com/wesmckinn/status/986998077767716865)\n",
    "- @wesmckinn has highest mean value for retweet count and favorite count\n",
    "\n",
    "Since @wesmckinn has also the highest followers count, how these stats change if we normalize the dataframe using the followers count?\n",
    "Obviously one tweet can get favorited/retweeted even from non-followers, but this normalization will probably produce more fair results because the higher the followers count, the most the tweet will probably be viewed.\n",
    "\n",
    "After the normalization we can see that @cournape and @teoliphant are getting higher mean values, in terms of retweets and favorites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General stats normalized by followers count\n",
    "tmp_df = df_tweets.merge(df_users[[\"screen_name\", \"followers_count\"]], left_on=\"username\", right_on=\"screen_name\")\n",
    "tmp_df[\"favorite_count perc\"] = tmp_df[\"favorite_count\"] / tmp_df[\"followers_count\"] * 100\n",
    "tmp_df[\"retweet_count perc\"] = tmp_df[\"retweet_count\"] / tmp_df[\"followers_count\"] * 100\n",
    "tmp_df = tmp_df.groupby(\"username\").agg({\"favorite_count perc\": [\"count\", \"max\", \"mean\", \"std\"],\n",
    "                                         \"retweet_count perc\": [\"max\", \"mean\", \"std\"]}).reset_index()\n",
    "tmp_df.columns = [' '.join(col).strip() for col in tmp_df.columns.values]\n",
    "\n",
    "tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how the monthly number of tweets changes over time, per user. From the following chart we can see for example that @kennethreitz tweeted a lot in september 2017 (more than 800 tweets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = df_tweets.set_index(\"created_at\").groupby([\"username\", pd.TimeGrouper(\"M\")]).size().reset_index()\n",
    "pv['created_at'] = pv['created_at'].apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "pv.rename(columns={\"id_str\": \"count\"}, inplace=True)\n",
    "pv = pd.pivot_table(pv, index=\"created_at\", columns=\"username\", aggfunc=\"sum\")\n",
    "pv.columns = pv.columns.get_level_values(1)\n",
    "print(pv)\n",
    "kwargs = dict(rot=45, title=\"Monthly tweets distribution over time\")\n",
    "_bar_plot(df=pv, show_graphs=show_graphs, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can even see which tools are used the most to tweet, per user; I grouped a lot of less used tools under \"Other\" (Tweetbot for iÎS, Twitter for iPad, OS X, Instagram, Foursquare, Facebook, LinkedIn, Squarespace, Medium, Buffer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = pd.pivot_table(df_tweets, values=\"id_str\", index=[\"username\"], columns=[\"source\"], aggfunc=\"count\")\n",
    "pv = pv[pv.sum().sort_values(ascending=False).index]\n",
    "pv = pv.div(pv.sum(1) / 100, 0)\n",
    "other_col = pv.columns[pv.sum() < 20]\n",
    "print(f\"Grouping columns under 'Other' label: {other_col}\")\n",
    "pv[\"Other\"] = pv[other_col].sum(1)\n",
    "pv.drop(other_col, axis=1, inplace=True)\n",
    "display(pv)\n",
    "kwargs = dict(stacked=True, edgecolor=\"black\", linewidth=0.5, colormap=\"tab20\", legend=\"reverse\", rot=45, title=\"Sources\")\n",
    "_bar_plot(df=pv, show_graphs=show_graphs, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can build a kind of punchcard chart for each user, showing an aggregation of tweets dates by day of the week and hours of the day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timezone could be read from the df_users, but I'm lazy...\n",
    "timezones = {'kennethreitz': 'US/Eastern', 'wesmckinn': 'US/Eastern', 'mitsuhiko': 'Europe/Vienna',\n",
    "             'asksol': 'US/Pacific', 'benoitc': 'Europe/Paris', 'cournape': 'Europe/Amsterdam',\n",
    "             'zzzek': 'America/Guayaquil', 'teoliphant': 'US/Central'}\n",
    "\n",
    "fig = plt.figure()\n",
    "i = 0\n",
    "for username, grouped_df in df_tweets.groupby(\"username\"):\n",
    "    print(f\"Preparing punch_card for {username}\")\n",
    "\n",
    "    grouped_df['created_at'] = grouped_df['created_at'].dt.tz_localize('UTC').dt.tz_convert(timezones.get(username))\n",
    "    grouped_df['weekday'] = grouped_df['created_at'].apply(lambda x: int(x.strftime(\"%w\")))\n",
    "    grouped_df['hour'] = grouped_df['created_at'].apply(lambda x: int(x.strftime(\"%H\")))\n",
    "    grouped_df = grouped_df.groupby([\"hour\", \"weekday\"]).size().reset_index()\n",
    "\n",
    "    ax = fig.add_subplot(2, 4, i + 1)\n",
    "    grouped_df.plot(kind='scatter', x='hour', y='weekday', s=grouped_df[0].values * 2.5, ax=ax)\n",
    "    # ax.xaxis.set_major_formatter(ticker.FixedFormatter(['', '00:00', '04:00', '08:00', '12:00', '16:00', '20:00']))\n",
    "    ax.yaxis.set_major_formatter(ticker.FixedFormatter(['', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']))\n",
    "    ax.set_title(f\"@{username}\")\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what are the devs in the list talking about?\n",
    "\n",
    "Let's start with a simple visualization, a word cloud.\n",
    "After some basic preprocessing of the text from standard tweets only (tokenization, pos tagging, stopwords removal, bigrams, etc), I grouped the tweets by username and got the most common words for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_dictionary_corpus(df_tweets: pd.DataFrame, min_count: int, no_above: float) -> Tuple[Dictionary, List, List]:\n",
    "\n",
    "    # lem = nltk.WordNetLemmatizer()\n",
    "    tokens_list = [x.split() for x in df_tweets[\"text_clean\"].tolist()]\n",
    "    tags_list = [nltk.pos_tag(token_list) for token_list in tokens_list]\n",
    "    docs = []\n",
    "    for tag_list in tags_list:\n",
    "        sentence = []\n",
    "        for word, tag in tag_list:\n",
    "            # word = lem.lemmatize(word)\n",
    "            if not word.isnumeric() and len(word) > 1 and word not in STOPWORDS:\n",
    "                sentence.append(word)\n",
    "        docs.append(sentence)\n",
    "\n",
    "    # p_stemmer = PorterStemmer()\n",
    "    # docs = [[p_stemmer.stem(token) for token in doc] for doc in docs]\n",
    "\n",
    "    ph = Phrases(docs, min_count=min_count)\n",
    "    bigram = Phraser(ph)\n",
    "    for doc in docs:\n",
    "        for token in bigram[doc]:\n",
    "            if \"_\" in token:\n",
    "                doc.append(token)\n",
    "\n",
    "    dictionary = Dictionary(docs)\n",
    "    dictionary.filter_extremes(no_below=min_count, no_above=no_above)\n",
    "    if dictionary:\n",
    "        __ = dictionary[0]  # This is only to \"load\" the dictionary\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "    return dictionary, corpus, docs\n",
    "\n",
    "\n",
    "df_wc = _check_condition(df_tweets, \n",
    "                         condition=~df_tweets[\"tweet_type\"].isin([\"retweet\", \"reply\"]), \n",
    "                         label=\"not RTs\")\n",
    "df_wc = _check_condition(df_wc, \n",
    "                         condition=df_wc[\"lang\"].isin([\"en\", \"und\"]), \n",
    "                         label=\"only english/undefined\")\n",
    "\n",
    "tweet_wc = WordCloud(background_color=\"white\", max_words=75, width=600, height=500,\n",
    "                     normalize_plurals=False, regexp=\"(#\\\\w+|@\\\\w+|\\\\w+)\", relative_scaling=0.6)\n",
    "\n",
    "fig = plt.figure()\n",
    "i = 0\n",
    "for username, grouped_df in df_wc.groupby(\"username\"):\n",
    "\n",
    "    dictionary, corpus, docs = _create_dictionary_corpus(df_tweets=grouped_df, min_count=20, no_above=0.5)\n",
    "    w_list = list(itertools.chain(*docs))\n",
    "    words = \" \".join(w_list)\n",
    "\n",
    "    print(f\"@{username}|{len(grouped_df)}|{Counter(w_list).most_common(6)}\")\n",
    "    ax = fig.add_subplot(2, 4, i + 1)\n",
    "    tweet_wc.generate(words)\n",
    "    ax.imshow(tweet_wc, interpolation=\"bilinear\")\n",
    "    ax.set_title(f\"@{username} ({len(grouped_df)} tweets)\")\n",
    "    ax.axis(\"off\")\n",
    "    i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to identify real topics, using an LDAmodel from Gensim. I still used standard tweets from two accounts only (@kennethreitz and @mitsuhiko) and I performed the same preprocessing used for wordclouds generation.\n",
    "I run the model using two dynamic values: the number of topics (ranging between 2 and 14) and the alpha value (with possible values 0.2, 0.3, 0.4).\n",
    "Then I chose the best model using the Gensim built-in Coherence Model, using c_v as a metric: the best model is the one with 9 topics and alpha=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topics(min_topics: int, max_topics: int, dictionary, corpus, texts, author2doc=None, coherence=\"c_v\"):\n",
    "\n",
    "    iterations = 100\n",
    "    passes = 20\n",
    "    chunksize = 10000\n",
    "    eval_every = 10\n",
    "    minimum_probability = 0.1\n",
    "    random_state = 1\n",
    "\n",
    "    alphas = (0.2, 0.3, 0.4)\n",
    "\n",
    "    c_v_map = {}\n",
    "\n",
    "    for num_topics in range(min_topics, max_topics):\n",
    "        for alpha in alphas:\n",
    "            lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                          alpha=alpha, iterations=iterations, passes=passes, chunksize=chunksize,\n",
    "                          eval_every=eval_every, minimum_probability=minimum_probability, random_state=random_state)\n",
    "\n",
    "            kwargs = dict(model=lm, corpus=corpus, dictionary=dictionary, coherence=coherence)\n",
    "            if coherence in (\"c_v\", \"c_uci\", \"c_npmi\"):\n",
    "                kwargs.update(dict(texts=texts))\n",
    "            cm = CoherenceModel(**kwargs)\n",
    "            tc_cv = cm.get_coherence()\n",
    "\n",
    "            c_v_map[f\"{alpha}_{num_topics}\"] = [lm, tc_cv]\n",
    "            print([num_topics, alpha, tc_cv])\n",
    "\n",
    "    # Show graph\n",
    "    for alpha in alphas:\n",
    "        plt.plot(range(min_topics, max_topics),\n",
    "                 [v[1] for k, v in c_v_map.items() if k.startswith(str(alpha))],\n",
    "                 label=alpha)\n",
    "    plt.xlabel(\"num_topics\")\n",
    "    plt.ylabel(f\"Coherence score ({coherence})\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "    return c_v_map\n",
    "\n",
    "\n",
    "df_topics = _check_condition(df_tweets,\n",
    "                             condition=~df_tweets[\"tweet_type\"].isin([\"retweet\", \"reply\"]),\n",
    "                             label=\"not RTs\")\n",
    "df_topics = _check_condition(df_topics,\n",
    "                             condition=df_topics[\"lang\"].isin([\"en\", \"und\"]),\n",
    "                             label=\"only english/undefined\")\n",
    "\n",
    "df_topics = _check_condition(df_topics,\n",
    "                             condition=df_topics[\"username\"].isin([\"kennethreitz\", \"mitsuhiko\"]),\n",
    "                             label=\"only kennethreitz and mitsuhiko\")\n",
    "\n",
    "usernames = set(df_topics[\"username\"].tolist())\n",
    "\n",
    "min_topics = 2\n",
    "max_topics = 15\n",
    "\n",
    "min_count = 10\n",
    "no_above = 0.5\n",
    "\n",
    "dictionary, corpus, docs = _create_dictionary_corpus(df_tweets=df_topics, min_count=min_count, no_above=no_above)\n",
    "\n",
    "print(f\"Number of unique authors: {len(usernames)}\")\n",
    "print(f\"Number of unique tokens: {len(dictionary)}\")\n",
    "print(f\"Number of documents: {len(corpus)}\")\n",
    "\n",
    "c_v_map = evaluate_topics(dictionary=dictionary, corpus=corpus, texts=docs, \n",
    "                          min_topics=min_topics, max_topics=max_topics, coherence=\"c_v\")\n",
    "\n",
    "selected = max(c_v_map.keys(), key=(lambda k: c_v_map[k][1]))\n",
    "selected_alpha, selected_num_topics = selected.split(\"_\")\n",
    "print(f\"Selected {selected_num_topics} num_topics in range ({min_topics}-{max_topics - 1}, with alpha={selected_alpha})\")\n",
    "model = c_v_map[f\"{selected_alpha}_{selected_num_topics}\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(model.show_topics(num_topics=-1, num_words=5)):\n",
    "    print(f\"{i}|{topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = gensimvis.prepare(model, corpus, dictionary)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and future steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post I showed how to get data from Twitter APIs and how to perform some simple analysis in order to know in advance some features about an account (e.g. tweet style, statistics about tweets, topics).\n",
    "Your mileage may vary depending on the initial account list and the configuration of the algorithms (especially in topics detection).\n",
    "\n",
    "Next steps:\n",
    "- Improve preprocessing using lemmatization and stemming\n",
    "- Try different algorithms for topics detection using Gensim (e.g. AuthorTopicModel or LDAMallet) or scikit-learn\n",
    "- Add sentiment analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
